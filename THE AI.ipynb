{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f9bd8-da14-41e6-963f-08aa9b1e6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://github.com/johntdavies/james/raw/main/data/files.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f340df-670b-48ba-8f1c-4981e5c51435",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar zxf files.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99027c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ff0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['cat_1','cat_2','cat_3','cat_4']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d99dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 77, 77, 4)\n",
      "(60000,)\n",
      "[53988 44335 13852 ... 32911 59229 22661]\n",
      "[3. 2. 0. ... 2. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "class_x = []\n",
    "class_y = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    a_folder = folders[i]\n",
    "\n",
    "    pattern = os.path.join(f'train/{a_folder}', '*.png')\n",
    "\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    train_x = []\n",
    "    for file in files:\n",
    "        img = imread(file)\n",
    "        #img = np.mean(img[:, :, :3], axis=-1)\n",
    "        # Normalize the image data\n",
    "        train_x.append(img)\n",
    "    train_x = np.array(train_x)\n",
    "    class_x.append(train_x)\n",
    "\n",
    "    train_y = (i) * np.ones(len(train_x))\n",
    "    class_y.append(train_y)\n",
    "\n",
    "\n",
    "final_x = np.concatenate((class_x[0],class_x[1],class_x[2],class_x[3]))#,class_x[4],class_x[5],class_x[6],class_x[7],class_x[8]))\n",
    "final_y = np.concatenate((class_y[0],class_y[1],class_y[2],class_y[3]))#,class_y[4],class_y[5],class_y[6],class_y[7],class_y[8]))\n",
    "\n",
    "print(final_x.shape)\n",
    "print(final_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "def shuffle(X,Y):\n",
    "    assert len(X) == len(Y)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    print(indices)\n",
    "    # Use the shuffled indices to reorder both X and Y consistently\n",
    "    X_shuffled = X[indices]\n",
    "    Y_shuffled = Y[indices]\n",
    "    print(Y_shuffled)\n",
    "\n",
    "    return X_shuffled,Y_shuffled\n",
    "\n",
    "\n",
    "X_train,y_train = shuffle(final_x,final_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4993fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14119, 77, 77, 4)\n",
      "(14119,)\n",
      "[1702 2411 2051 ... 5988 6729 8754]\n",
      "[0. 0. 0. ... 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "class_x = []\n",
    "class_y = []\n",
    "\n",
    "folders2 = []\n",
    "for i in range(0,4):\n",
    "    a_folder = folders[i]\n",
    "\n",
    "    pattern = os.path.join(f'test/{a_folder}', '*.png')\n",
    "\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    train_x = []\n",
    "    for file in files:\n",
    "        img = imread(file)\n",
    "        #img = np.mean(img[:, :, :3], axis=-1)\n",
    "        # Normalize the image data\n",
    "        train_x.append(img)\n",
    "    train_x = np.array(train_x)\n",
    "    class_x.append(train_x)\n",
    "\n",
    "    train_y = (i) * np.ones(len(train_x))\n",
    "    class_y.append(train_y)\n",
    "\n",
    "\n",
    "final_x = np.concatenate((class_x[0],class_x[1],class_x[2],class_x[3]))#,class_x[4],class_x[5],class_x[6],class_x[7],class_x[8]))\n",
    "final_y = np.concatenate((class_y[0],class_y[1],class_y[2],class_y[3]))#,class_y[4],class_y[5],class_y[6],class_y[7],class_y[8]))\n",
    "\n",
    "print(final_x.shape)\n",
    "print(final_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "def shuffle(X,Y):\n",
    "    assert len(X) == len(Y)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    print(indices)\n",
    "    # Use the shuffled indices to reorder both X and Y consistently\n",
    "    X_shuffled = X[indices]\n",
    "    Y_shuffled = Y[indices]\n",
    "    print(Y_shuffled)\n",
    "\n",
    "    return X_shuffled,Y_shuffled\n",
    "\n",
    "X_test,y_test = shuffle(final_x,final_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3dca8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_proper = to_categorical(y_train)\n",
    "y_test_proper = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c62684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proper = X_train.reshape(60000,77,77,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b75da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proper = X_test.reshape(14119,77,77,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05247501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9c270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Second convolution layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(2,2), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Third convolution layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Fourth convolution layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Flattening the convolutions\n",
    "model.add(Flatten())\n",
    "\n",
    "# First Dense layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Second Dense layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb20701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df95f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 240ms/step - accuracy: 0.2818 - loss: 1.4147 - val_accuracy: 0.2629 - val_loss: 1.4265\n",
      "Epoch 2/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 244ms/step - accuracy: 0.3079 - loss: 1.3623 - val_accuracy: 0.2800 - val_loss: 1.3889\n",
      "Epoch 3/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 247ms/step - accuracy: 0.3324 - loss: 1.3326 - val_accuracy: 0.2898 - val_loss: 1.4417\n",
      "Epoch 4/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 240ms/step - accuracy: 0.3617 - loss: 1.2949 - val_accuracy: 0.2857 - val_loss: 1.4789\n",
      "Epoch 5/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 240ms/step - accuracy: 0.4298 - loss: 1.2083 - val_accuracy: 0.2791 - val_loss: 1.6661\n",
      "Epoch 6/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 241ms/step - accuracy: 0.5154 - loss: 1.0862 - val_accuracy: 0.2675 - val_loss: 1.6966\n",
      "Epoch 7/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 243ms/step - accuracy: 0.5939 - loss: 0.9406 - val_accuracy: 0.2847 - val_loss: 2.2344\n",
      "Epoch 8/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 242ms/step - accuracy: 0.6634 - loss: 0.8101 - val_accuracy: 0.2871 - val_loss: 1.9901\n",
      "Epoch 9/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 243ms/step - accuracy: 0.7187 - loss: 0.7007 - val_accuracy: 0.2815 - val_loss: 2.3427\n",
      "Epoch 10/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 245ms/step - accuracy: 0.7555 - loss: 0.6182 - val_accuracy: 0.2785 - val_loss: 2.4475\n",
      "Epoch 11/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 242ms/step - accuracy: 0.7839 - loss: 0.5475 - val_accuracy: 0.2701 - val_loss: 2.9782\n",
      "Epoch 12/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 242ms/step - accuracy: 0.8073 - loss: 0.4927 - val_accuracy: 0.2707 - val_loss: 2.4541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_proper,y_train_proper,epochs=30,validation_data=(X_test_proper,y_test_proper),batch_size=128,callbacks=early_stop)\n",
    "model.save('PredictiveModel1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.DataFrame(model.history.history)\n",
    "metrics[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions = model.predict(X_test_proper)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
