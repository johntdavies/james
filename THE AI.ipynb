{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99027c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ff0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['cat_1','cat_2','cat_3','cat_4']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d99dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 77, 77, 4)\n",
      "(60000,)\n",
      "[53988 44335 13852 ... 32911 59229 22661]\n",
      "[3. 2. 0. ... 2. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "class_x = []\n",
    "class_y = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    a_folder = folders[i]\n",
    "\n",
    "    pattern = os.path.join(f'train/{a_folder}', '*.png')\n",
    "\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    train_x = []\n",
    "    for file in files:\n",
    "        img = imread(file)\n",
    "        #img = np.mean(img[:, :, :3], axis=-1)\n",
    "        # Normalize the image data\n",
    "        train_x.append(img)\n",
    "    train_x = np.array(train_x)\n",
    "    class_x.append(train_x)\n",
    "\n",
    "    train_y = (i) * np.ones(len(train_x))\n",
    "    class_y.append(train_y)\n",
    "\n",
    "\n",
    "final_x = np.concatenate((class_x[0],class_x[1],class_x[2],class_x[3]))#,class_x[4],class_x[5],class_x[6],class_x[7],class_x[8]))\n",
    "final_y = np.concatenate((class_y[0],class_y[1],class_y[2],class_y[3]))#,class_y[4],class_y[5],class_y[6],class_y[7],class_y[8]))\n",
    "\n",
    "print(final_x.shape)\n",
    "print(final_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "def shuffle(X,Y):\n",
    "    assert len(X) == len(Y)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    print(indices)\n",
    "    # Use the shuffled indices to reorder both X and Y consistently\n",
    "    X_shuffled = X[indices]\n",
    "    Y_shuffled = Y[indices]\n",
    "    print(Y_shuffled)\n",
    "\n",
    "    return X_shuffled,Y_shuffled\n",
    "\n",
    "\n",
    "X_train,y_train = shuffle(final_x,final_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4993fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14119, 77, 77, 4)\n",
      "(14119,)\n",
      "[1702 2411 2051 ... 5988 6729 8754]\n",
      "[0. 0. 0. ... 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "class_x = []\n",
    "class_y = []\n",
    "\n",
    "folders2 = []\n",
    "for i in range(0,4):\n",
    "    a_folder = folders[i]\n",
    "\n",
    "    pattern = os.path.join(f'test/{a_folder}', '*.png')\n",
    "\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    train_x = []\n",
    "    for file in files:\n",
    "        img = imread(file)\n",
    "        #img = np.mean(img[:, :, :3], axis=-1)\n",
    "        # Normalize the image data\n",
    "        train_x.append(img)\n",
    "    train_x = np.array(train_x)\n",
    "    class_x.append(train_x)\n",
    "\n",
    "    train_y = (i) * np.ones(len(train_x))\n",
    "    class_y.append(train_y)\n",
    "\n",
    "\n",
    "final_x = np.concatenate((class_x[0],class_x[1],class_x[2],class_x[3]))#,class_x[4],class_x[5],class_x[6],class_x[7],class_x[8]))\n",
    "final_y = np.concatenate((class_y[0],class_y[1],class_y[2],class_y[3]))#,class_y[4],class_y[5],class_y[6],class_y[7],class_y[8]))\n",
    "\n",
    "print(final_x.shape)\n",
    "print(final_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "def shuffle(X,Y):\n",
    "    assert len(X) == len(Y)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    print(indices)\n",
    "    # Use the shuffled indices to reorder both X and Y consistently\n",
    "    X_shuffled = X[indices]\n",
    "    Y_shuffled = Y[indices]\n",
    "    print(Y_shuffled)\n",
    "\n",
    "    return X_shuffled,Y_shuffled\n",
    "\n",
    "X_test,y_test = shuffle(final_x,final_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb35197f-4b86-43cc-9999-add101bd49b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (23.5.26)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (4.25.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (1.62.2)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jdavies/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 gast-0.5.4 google-pasta-0.2.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3dca8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_proper = to_categorical(y_train)\n",
    "y_test_proper = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c62684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proper = X_train.reshape(60000,77,77,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b75da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proper = X_test.reshape(14119,77,77,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05247501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa9c270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Second convolution layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(2,2), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Third convolution layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Fourth convolution layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Flattening the convolutions\n",
    "model.add(Flatten())\n",
    "\n",
    "# First Dense layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Second Dense layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb20701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df95f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 240ms/step - accuracy: 0.2818 - loss: 1.4147 - val_accuracy: 0.2629 - val_loss: 1.4265\n",
      "Epoch 2/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 244ms/step - accuracy: 0.3079 - loss: 1.3623 - val_accuracy: 0.2800 - val_loss: 1.3889\n",
      "Epoch 3/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 247ms/step - accuracy: 0.3324 - loss: 1.3326 - val_accuracy: 0.2898 - val_loss: 1.4417\n",
      "Epoch 4/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 240ms/step - accuracy: 0.3617 - loss: 1.2949 - val_accuracy: 0.2857 - val_loss: 1.4789\n",
      "Epoch 5/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 240ms/step - accuracy: 0.4298 - loss: 1.2083 - val_accuracy: 0.2791 - val_loss: 1.6661\n",
      "Epoch 6/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 241ms/step - accuracy: 0.5154 - loss: 1.0862 - val_accuracy: 0.2675 - val_loss: 1.6966\n",
      "Epoch 7/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 243ms/step - accuracy: 0.5939 - loss: 0.9406 - val_accuracy: 0.2847 - val_loss: 2.2344\n",
      "Epoch 8/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 242ms/step - accuracy: 0.6634 - loss: 0.8101 - val_accuracy: 0.2871 - val_loss: 1.9901\n",
      "Epoch 9/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 243ms/step - accuracy: 0.7187 - loss: 0.7007 - val_accuracy: 0.2815 - val_loss: 2.3427\n",
      "Epoch 10/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 245ms/step - accuracy: 0.7555 - loss: 0.6182 - val_accuracy: 0.2785 - val_loss: 2.4475\n",
      "Epoch 11/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 242ms/step - accuracy: 0.7839 - loss: 0.5475 - val_accuracy: 0.2701 - val_loss: 2.9782\n",
      "Epoch 12/30\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 242ms/step - accuracy: 0.8073 - loss: 0.4927 - val_accuracy: 0.2707 - val_loss: 2.4541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_proper,y_train_proper,epochs=30,validation_data=(X_test_proper,y_test_proper),batch_size=128,callbacks=early_stop)\n",
    "model.save('PredictiveModel1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.DataFrame(model.history.history)\n",
    "metrics[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions = model.predict(X_test_proper)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
